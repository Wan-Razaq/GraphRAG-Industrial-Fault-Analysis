{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting fuzzywuzzy\n",
      "  Downloading fuzzywuzzy-0.18.0-py2.py3-none-any.whl.metadata (4.9 kB)\n",
      "Collecting python-Levenshtein\n",
      "  Downloading python_levenshtein-0.27.1-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting Levenshtein==0.27.1 (from python-Levenshtein)\n",
      "  Downloading levenshtein-0.27.1-cp312-cp312-macosx_11_0_arm64.whl.metadata (3.6 kB)\n",
      "Collecting rapidfuzz<4.0.0,>=3.9.0 (from Levenshtein==0.27.1->python-Levenshtein)\n",
      "  Downloading rapidfuzz-3.13.0-cp312-cp312-macosx_11_0_arm64.whl.metadata (12 kB)\n",
      "Downloading fuzzywuzzy-0.18.0-py2.py3-none-any.whl (18 kB)\n",
      "Downloading python_levenshtein-0.27.1-py3-none-any.whl (9.4 kB)\n",
      "Downloading levenshtein-0.27.1-cp312-cp312-macosx_11_0_arm64.whl (156 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m156.4/156.4 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading rapidfuzz-3.13.0-cp312-cp312-macosx_11_0_arm64.whl (1.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: fuzzywuzzy, rapidfuzz, Levenshtein, python-Levenshtein\n",
      "Successfully installed Levenshtein-0.27.1 fuzzywuzzy-0.18.0 python-Levenshtein-0.27.1 rapidfuzz-3.13.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install fuzzywuzzy python-Levenshtein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fuzzywuzzy import fuzz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_json(path: Path) -> List[Dict]:\n",
    "    \"\"\"Load a JSON file and return a list of dictionaries.\"\"\"\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        return json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_entities_detailed(human: List[Dict], extracted: List[Dict]) -> Dict[str, Any]:\n",
    "    detailed_results = {}\n",
    "\n",
    "    def compare_items(h_items, e_items, fuzzy_threshold=85):\n",
    "        exact_matches, mismatches, missing, hallucinations = set(), [], set(), set()\n",
    "\n",
    "        h_set = set(i.lower().strip() for i in h_items if i.strip())\n",
    "        e_set = set(i.lower().strip() for i in e_items if i.strip())\n",
    "\n",
    "        exact = h_set & e_set\n",
    "        exact_matches.update(exact)\n",
    "\n",
    "        miss = h_set - e_set\n",
    "        extra = e_set - h_set\n",
    "\n",
    "        # Improved mismatch detection: fuzzy match\n",
    "        for m in miss.copy():\n",
    "            best_score = 0\n",
    "            best_e = None\n",
    "            for e in extra:\n",
    "                score = fuzz.partial_ratio(m, e)\n",
    "                if score > best_score:\n",
    "                    best_score = score\n",
    "                    best_e = e\n",
    "\n",
    "            if best_score >= fuzzy_threshold:\n",
    "                mismatches.append((m, best_e))\n",
    "                miss.discard(m)\n",
    "                extra.discard(best_e)\n",
    "\n",
    "        missing.update(miss)\n",
    "        hallucinations.update(extra)\n",
    "\n",
    "        return {\n",
    "            \"exact_matches\": list(exact_matches),\n",
    "            \"mismatches\": mismatches,\n",
    "            \"missing\": list(missing),\n",
    "            \"hallucinations\": list(hallucinations)\n",
    "        }\n",
    "\n",
    "    for human_case in human:\n",
    "        extracted_case = next((ec for ec in extracted if ec[\"case_id\"] == human_case[\"case_id\"]), None)\n",
    "        if not extracted_case:\n",
    "            continue\n",
    "\n",
    "        h_res, e_res = human_case[\"result\"], extracted_case[\"result\"]\n",
    "\n",
    "        detailed_results[human_case[\"case_id\"]] = {\n",
    "            \"fault_location\": compare_items(\n",
    "                [f\"{h_res['fault_location']['name']}_{h_res['fault_location']['machine']}\"],\n",
    "                [f\"{e_res['fault_location']['name']}_{e_res['fault_location']['machine']}\"]\n",
    "            ),\n",
    "            \"fault_symptoms\": compare_items(\n",
    "                h_res[\"fault_symptoms\"],\n",
    "                e_res[\"fault_symptoms\"]\n",
    "            ),\n",
    "            \"fault_reasons\": compare_items(\n",
    "                [r[\"name\"] for r in h_res[\"fault_reason\"]],\n",
    "                [r[\"name\"] for r in e_res[\"fault_reason\"]]\n",
    "            ),\n",
    "            \"fault_measures\": compare_items(\n",
    "                [m[\"description\"] for m in h_res[\"fault_measures\"]],\n",
    "                [m[\"description\"] for m in e_res[\"fault_measures\"]]\n",
    "            ),\n",
    "            \"resolution_status\": compare_items(\n",
    "                [h_res[\"resolution_status\"]],\n",
    "                [e_res[\"resolution_status\"]]\n",
    "            ),\n",
    "        }\n",
    "\n",
    "    return detailed_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation Summary\n",
      "============================\n",
      "Exact Matches: 68 (51.5%)\n",
      "Mismatches: 23 (17.4%)\n",
      "Missing: 21 (15.9%)\n",
      "Potential Hallucinations: 20 (15.2%)\n",
      "\n",
      "✅ Per-case evaluation details saved as 'per_case_evaluation_details.json'\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    human_annotations_path = Path(\"/Users/wbm/Documents/BIT/Research Topics/Evaluation Knowledge Extraction/formatted_annotated_cases_2.json\")\n",
    "    extracted_output_path = Path(\"/Users/wbm/Documents/BIT/Research Topics/few-shot-prompting/baml_extracted_20_cases.json\")\n",
    "\n",
    "    human_data = load_json(human_annotations_path)\n",
    "    extracted_data = load_json(extracted_output_path)\n",
    "\n",
    "    \n",
    "    detailed_results = evaluate_entities_detailed(human_data, extracted_data)\n",
    "\n",
    "    # Optional: aggregate metrics for summary\n",
    "    total_exact = total_mismatch = total_missing = total_hallucinations = 0\n",
    "\n",
    "    for case_id, metrics in detailed_results.items():\n",
    "        for entity_type, entity_metrics in metrics.items():\n",
    "            total_exact += len(entity_metrics[\"exact_matches\"])\n",
    "            total_mismatch += len(entity_metrics[\"mismatches\"])\n",
    "            total_missing += len(entity_metrics[\"missing\"])\n",
    "            total_hallucinations += len(entity_metrics[\"hallucinations\"])\n",
    "\n",
    "    total = total_exact + total_mismatch + total_missing + total_hallucinations\n",
    "\n",
    "    print(\"\\nEvaluation Summary\")\n",
    "    print(\"============================\")\n",
    "    print(f\"Exact Matches: {total_exact} ({total_exact/total:.1%})\")\n",
    "    print(f\"Mismatches: {total_mismatch} ({total_mismatch/total:.1%})\")\n",
    "    print(f\"Missing: {total_missing} ({total_missing/total:.1%})\")\n",
    "    print(f\"Potential Hallucinations: {total_hallucinations} ({total_hallucinations/total:.1%})\")\n",
    "\n",
    "    # Save detailed results for manual checking\n",
    "    with open(\"per_case_evaluation_details.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(detailed_results, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "    print(\"\\n✅ Per-case evaluation details saved as 'per_case_evaluation_details.json'\")\n",
    "\n",
    "main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "def load_json(path: Path) -> list:\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "# Load the two files\n",
    "human_data = load_json(Path(\"/Users/wbm/Documents/BIT/Research Topics/Evaluation Knowledge Extraction/formatted_annotated_cases_2.json\"))\n",
    "extracted_data = load_json(Path(\"/Users/wbm/Documents/BIT/Research Topics/few-shot-prompting/baml_extracted_20_cases.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Per-case evaluation details saved!\n"
     ]
    }
   ],
   "source": [
    "detailed_results = evaluate_entities_detailed(human_data, extracted_data)\n",
    "\n",
    "# Save for manual checking\n",
    "with open(\"per_case_evaluation_details.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(detailed_results, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(\"✅ Per-case evaluation details saved!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
